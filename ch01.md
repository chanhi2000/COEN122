# ch01

- #### Abstraction
- #### What we will learn
- #### Eight Great ideas
- #### Performance
- #### From Uniprocessors to multiprocessor

## THE COMPUTER REVOLUTION
- Progress in computer technology
	- Underpinned by Moore’s Law
- Makes novel applications feasible
	- Computers in automobiles
	- Cell phones
	- Human genome project
	- World Wide Web
	- Search Engines

## CLASSES OF COMPUTERS
- #### Personal computers
	- General purpose, variety of software
	- Subject to cost/performance tradeoff
- #### Server computers
	- Network based
	- High capacity, performance, reliability
	- Range from small servers to building sized
- #### Supercomputers
	- High-end scientific and engineering calculations
	- Highest capability but represent a small fraction of the overall computer market
- #### Embedded computers
	- Hidden as components of systems
	- Stringent power/performance/cost constraints

## The PostPC Era
![fig01]()

- #### Personal Mobile Device (PMD)
	- Battery operated
	- Connects to the Internet
	- Hundreds of dollars
	- Smart phones, tablets, electronic glasses
- #### Cloud computing
	- **W**arehouse **S**cale **C**omputers (WSC)
	- **S**oftware **a**s **a** **S**ervice (SaaS)
	- Portion of software run on a PMD and a portion run in the Cloud
	- Amazon and Google

## WHAT YOU WILL LEARN
- How the hardware executes machine code
- The hardware/software interface
- What determines program performance
	- And how it can be improved
- How hardware designers improve performance
- What is parallel processing

## UNDERSTANDING PERFORMANCE
- #### Algorithm
	- Determines number of operations executed
- #### Programming language, compiler, architecture
	- Determine number of machine instructions executed per operation
- #### Processor and memory system
	- Determine how fast instructions are executed
- #### I/O system (including OS)
	- Determines how fast I/O operations are executed

## EIGHT GREAT IDEAS
- Design for **Moore’s Law**
- Use **abstraction** to simplify design
- Make the **common case fast**
- Performance via **parallelism**
- Performance via **pipelining**
- Performance via **prediction**
- **Hierarchy** of memories
- **Dependability** via redundancy


## BELOW YOUR PROGRAM
- Application software
	- Written in high-level language
- System software
	- Compiler: translates HLL code tomachine code
	- Operating System: service code
		- Handling input/output
		- Managing memory and storage
		- Scheduling tasks & sharing resources
- Hardware
	- Processor, memory, I/O controllers


## LEVELS OF PROGRAM CODE
- High-level language
	- Level of abstraction closer to problem domain
	- Provides for productivity and portability
- Assembly language
	- Textual representation of instructions
- Hardware representation
	- Binary digits (bits)
	- Encoded instructions and data

## COMPONENTS OF A COMPUTER
- Same components for all kinds of computer
	- Desktop, server, embedded
- Input/output includes
	- User-interface devices
		- Display, keyboard, mouse
	- Storage devices
		- Hard disk, CD/DVD, flash
	- Network adapters
		- For communicating with other computers

## INSIDE THE PROCESSOR (CPU)
- #### Datapath:
	- performs operations on data
- #### Control:
	- sequences datapath, memory, ...
- #### Cache memory
	- Small fast SRAM memory for immediate access to data


## ABSTRACTIONS
- Abstraction helps us deal with complexity
	- Hide lower-level detail
- One important abstraction: **Instruction set architecture** (ISA)
	- hardware/software interface
	- includes anything a programmer needs to know to make a binary machine language work
		- \# instructions,
		- formats,
		- addressing modes,
		- …
	- decides workload b/w software & hardware
- **An implementation**: hardware that obeys the architecture abstract
	- An architecture can have several different implementations
- Examples of instruction set architectures:
	- MIPS, SPARC, ARM, and others


## POWER VS. PERFORMANCE
$$
\text{POWER}=\left(\begin{matrix}\text{capacitive}\\\text{load}\end{matrix}\right)\left(\begin{matrix}\text{voltage}\end{matrix}\right)^2\left(\begin{matrix}\text{frequency}\\text{switched}\end{matrix}\right)
$$
- **Power** is a linear function of clock rate
- **Performance** is a linear function of clock rate
- Power issues: overheating and PMD
- Resolving power issues in past:
	- Decrease voltage $$\to\:5\:\text{V}\to1\:\text{V}$$ in 20 years
	- Cooling – remove heat
- The power wall
	- We can’t reduce voltage further
	- We can’t remove more heat
- How else can we improve performance?
- **One possible solution**: multi cores
- Power is a function of $$\sqrt{\text{area}}=\sqrt{\#\text{cores}}$$
- After 2006, desktops and severs are with multi core


## MICROPROCESSORS
- multicore-microprocessor
	- More than one processor per chip
- Requires explicitly parallel programming
	- Compare with instruction level parallelism
		- Hardware executes multiple instructions at once
		- Hidden from the programmer
	- Hard to do
		- Programming for performance
		- Load balancing
		- Optimizing communication and synchronization

## RESPONSE TIME & THROUGHPUT
- #### Response time
	- How long it takes to do a task
- #### Throughput
	- Total work done per unit time
		- *e.g.*, tasks/transactions/… per hour
- How are response time and throughput affected by
	- Replacing the processor with a faster version?
	- Adding more processors?
- We’ll focus on response time for now…


## RELATIVE PERFORMANCE
$$\text{performance}=\frac{1}{\text{exec. time}}$$
- “$$x$$ is $$n$$ time faster than $$y$$”
	- $$\frac{\text{perf}(x)}{\text{perf}(y)}=\frac{t_{\text{exec}}(y)}{t_{\text{exec}}(x)}=n$$
- **Example**: time taken to run a program:
	- $$10\:\text{s}$$ on A,
    - $$15\:\text{s}$$ on B

$$
n=\frac{\text{perf}(A)}{\text{perf}(B)}=\frac{\frac{1}{10}}{\frac{1}{15}}=1.5
$$


## MEASURING EXECUTION TIME
- #### Elapsed time
	- Total response time, including all aspects
		- Processing, I/O, OS overhead, idle time
	- Determines system performance
- #### CPU time
	- Time spent processing a given job
		- Discounts I/O time, other jobs’ shares

## CPU CLOCKING
- Operation of digital hardware governed by a constant-rate clock

- **Clock period**: duration of a clock cycle
	- e.g., $$l_{\text{cycle}}=\frac{1}{r_{\text{CLK}}}$$
- **Clock frequency**: cycles per second
 	- e.g., $$r_{\text{CLK}}=\frac{\#\:\text{cycles}}{\text{sec}}$$
	$$
	\begin{align*}
	r_{\text{CLK}}&=4\:\text{GHz}=4\times10^{9}\:\frac{\text{cycles}}{\text{sec}}\\
	l_{\text{cycle}}&=\frac{1}{r_{\text{CLK}}}=\frac{1}{(4\times10^9)}=250\:\text{ps}
	\end{align*}
	$$

## CLOCK CYCLES vs. CLOCK RATE?
- Clock period = Cycle time= Cycle length
- Clock rate = Clock frequency
- How they are related?
$$
r_{\text{CLK}}=\frac{1}{l_{\text{cycle}}}
$$


## CPU TIME
$$
\begin{align*}
\text{CPU Time}&=\left(\#\:\text{cycles}\right)\left(t_{\text{cycle}}\right)\:\:\:\:\left<r_{\text{CLK}}=\frac{1}{l_{\text{cycle}}}\right>\\
&=\frac{\#\:\text{cycles}}{(r_{\text{CLK}})}
\end{align*}
$$
- Performance improved by
	- Reducing number of clock cycles
	- Increasing clock rate
	- Hardware designer must often trade off clock
	- rate against cycle count

## EXAMPLE: CPU TIME
- Computer A: $$2\:\text{GHz}$$ clock, $$10\:\text{s}$$ CPU time
- Designing Computer B
	- Aim for $$6\:\text{s}$$ CPU time
	- Can do faster clock, but causes $$1.2$$ times clock cycles
- How fast must Computer B clock be?

$$
\begin{align*}
r_{\text{CLK}}(A)&=2\text{GHz}=2\times10^9\:\text{Hz}\\
\text{CPU time}(A)&=10\:\text{sec}\\
\text{CPU time}(B)&=6\:\text{sec}\\
r_{\text{CLK}}(B)&=\frac{(\#\:\text{cycles}(B))}{\text{CPU time(B)}}=\frac{(1.2)(\#\:\text{cycles}(A))}{\text{CPU time(B)}}\:\:\:\:\left<\#\:\text{cycles}=(\text{CPU time})(r_{\text{CLK}})\right>\\
&=\frac{(1.2)(\text{CPU time}(A))(r_{\text{CLK}}(A))}{\text{CPU time(B)}}=\frac{(1.2)(10)(2\times10^9)}{(6)}\\
&=4\times10^9\:\text{Hzs}\\
\end{align*}
$$




